# üßë‚Äçüíª Technical Skills Rating System (1‚Äì10)

This repository provides a **universal, evidence-based system** to rate technical skills
(programming languages, libraries, tools, platforms) on a **1‚Äì10 scale**.

# Technical Skills Rating System (1‚Äì10)

A unified, evidence-based system to rate technical skills (programming languages, libraries, tools, platforms) on a 1‚Äì10 scale.

This repository includes:
- `RATING_SYSTEM.md` ‚Äî full rubric and guidance.
- `templates/skill_template.csv` ‚Äî CSV template for manual scoring.
- `templates/*.json` ‚Äî example JSON inputs.
- `code/evaluate.py` ‚Äî single evaluator script that auto-detects CSV / JSON / YAML.
- `notebooks/SkillRatingDemo.ipynb` ‚Äî demo notebook (optional).
- `examples/` ‚Äî sample human-readable outputs.

## Quick start

1. Clone & prepare
git clone https://github.com/mepsrajput/Technical-Skills-Rating-System.git
cd Technical-Skills-Rating-System


(Optional) create a virtual environment and install recommended deps:

python -m venv .venv
# Linux / Mac
source .venv/bin/activate
# Windows (PowerShell)
.venv\Scripts\Activate.ps1

pip install pandas pyyaml

2. Choose or create a template

Use an existing file in templates/ (e.g. sql_template.json, skill_template.csv), or

Create a new JSON / CSV / YAML file for the skill you want to evaluate.

Example JSON structure (templates/my_skill.json):

{
  "skill": "SkillName",
  "categories": [
    {"name": "Category A", "weight": 30, "score": 7},
    {"name": "Category B", "weight": 30, "score": 8},
    {"name": "Category C", "weight": 40, "score": 6}
  ]
}


Example CSV header (templates/skill_template.csv):

Skill,Category,Weight (%),Score (0-10)
MySkill,Core Knowledge,40,7
MySkill,Tooling,30,8
MySkill,Practice,30,6


If you create a new template, put it in templates/ so it‚Äôs easy to find and version.

3. Run the evaluator

Supported input types: .json, .csv, .yml / .yaml.

# basic run
python code/evaluate.py templates/my_skill.json

# save the detailed result
python code/evaluate.py templates/my_skill.json --output results/my_skill_result.json

# normalize weights to sum to 100 if needed
python code/evaluate.py templates/my_skill.csv --normalize --output results/my_skill_result.csv

4. Read the output

Console shows:

Weighted total (0‚Äì10)

Final rating (1‚Äì10, rounded)

Category breakdown (weight, score, weighted score)

If --output used, a detailed JSON/CSV is saved to results/.

5. Attach evidence (recommended)

Place proof files for high-confidence scores (especially scores ‚â• 7) in a results subfolder:

results/<skillname>/
‚îú‚îÄ queries/         # .sql or .py or code samples
‚îú‚îÄ explain/         # explain/analyze outputs or logs
‚îú‚îÄ benchmarks/      # timing CSVs or markdown
‚îî‚îÄ evidence.md      # short description + links

6. If a template does not exist

Copy an existing template as a starting point:

cp templates/skill_template.csv templates/<your_skill>.csv


Edit categories/weights/scores to match the skill.

Run the evaluator as above.

Rating Legend (quick)

1‚Äì3 ‚Äî Beginner

4‚Äì6 ‚Äî Practitioner (working/independent)

7‚Äì8 ‚Äî Expert (scales & optimizes)

9‚Äì10 ‚Äî Authority (public contributions, leadership)
